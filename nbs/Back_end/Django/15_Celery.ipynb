{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ab5797-1048-4463-9137-42957e9bbefe",
   "metadata": {},
   "source": [
    "# Celery\n",
    "\n",
    "> **Celery** is an asynchronous task queue/job queue system based on distributed message passing. It’s widely used in Django and other Python web frameworks to manage background tasks, perform distributed computation, and handle scheduled tasks. Celery is particularly powerful for running time-consuming tasks asynchronously to improve user experience and system responsiveness.\n",
    "\n",
    "\n",
    "- skip_showdoc: true\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6ef97a-58db-4717-9fef-de2328f5253f",
   "metadata": {},
   "source": [
    "### 1. **What is Celery?**\n",
    "\n",
    "**Celery** is a distributed system to process messages asynchronously by running tasks in the background. It uses a **message broker** to deliver messages between the main application and worker nodes. It is designed to handle distributed task processing at scale.\n",
    "\n",
    "### Key Features:\n",
    "- **Asynchronous Task Execution**: Run tasks in the background, freeing up your main application.\n",
    "- **Task Scheduling**: Execute tasks at specific intervals or after certain delays (like cron jobs).\n",
    "- **Distributed Processing**: Run tasks across multiple worker nodes for scalability and fault tolerance.\n",
    "- **Task Retrying**: Tasks can be automatically retried if they fail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f3002-5b55-4152-8a6b-1568204808a2",
   "metadata": {},
   "source": [
    "### 2. **Celery Architecture**\n",
    "\n",
    "#### a. **Workers**\n",
    "Workers are background processes that execute tasks. You can have multiple workers across different machines to handle tasks in parallel.\n",
    "\n",
    "#### b. **Message Broker**\n",
    "A message broker (such as **Redis**, **RabbitMQ**, or **Amazon SQS**) is used to send messages from your main application to Celery workers. The broker handles task queuing and routing.\n",
    "\n",
    "#### c. **Result Backend**\n",
    "The result backend stores the results of tasks. Celery supports several backends like **Redis**, **Django ORM**, **AMQP**, **Memcached**, and **SQLAlchemy**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1170efe-9edd-4bd8-9e31-452858608907",
   "metadata": {},
   "source": [
    "### 3. **Installation and Setup**\n",
    "\n",
    "#### a. **Install Celery**\n",
    "\n",
    "To install Celery in your project, use `pip`:\n",
    "\n",
    "```bash\n",
    "pip install celery\n",
    "```\n",
    "\n",
    "#### b. **Basic Celery Setup for a Django Project**\n",
    "\n",
    "1. **Create a `celery.py` file** in your Django project directory (where `settings.py` resides):\n",
    "\n",
    "   ```python\n",
    "   # project/celery.py\n",
    "   from __future__ import absolute_import, unicode_literals\n",
    "   import os\n",
    "   from celery import Celery\n",
    "\n",
    "   # set the default Django settings module for the 'celery' program.\n",
    "   os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'project.settings')\n",
    "\n",
    "   app = Celery('project')\n",
    "\n",
    "   # Using a string here means the worker doesn’t have to serialize\n",
    "   # the configuration object to child processes.\n",
    "   app.config_from_object('django.conf:settings', namespace='CELERY')\n",
    "\n",
    "   # Load task modules from all registered Django app configs.\n",
    "   app.autodiscover_tasks()\n",
    "   ```\n",
    "\n",
    "2. **Update `__init__.py`** to ensure that Celery is imported when Django starts.\n",
    "\n",
    "   ```python\n",
    "   # project/__init__.py\n",
    "\n",
    "   from __future__ import absolute_import, unicode_literals\n",
    "\n",
    "   # This will make sure the app is always imported when\n",
    "   # Django starts so that shared_task will use this app.\n",
    "   from .celery import app as celery_app\n",
    "\n",
    "   __all__ = ('celery_app',)\n",
    "   ```\n",
    "\n",
    "3. **Configure Celery in `settings.py`**:\n",
    "\n",
    "   Add the configuration for the message broker (for example, Redis):\n",
    "\n",
    "   ```python\n",
    "   # project/settings.py\n",
    "\n",
    "   # Celery settings\n",
    "   CELERY_BROKER_URL = 'redis://localhost:6379/0'  # Redis as message broker\n",
    "   CELERY_RESULT_BACKEND = 'redis://localhost:6379/0'  # Redis to store task results\n",
    "   CELERY_ACCEPT_CONTENT = ['json']\n",
    "   CELERY_TASK_SERIALIZER = 'json'\n",
    "   CELERY_RESULT_SERIALIZER = 'json'\n",
    "   CELERY_TIMEZONE = 'UTC'\n",
    "   ```\n",
    "\n",
    "#### c. **Starting Celery Workers**\n",
    "\n",
    "Once you’ve configured Celery, you need to run the Celery workers to process the tasks. Run this command from the Django project root:\n",
    "\n",
    "```bash\n",
    "celery -A project worker --loglevel=info\n",
    "```\n",
    "\n",
    "This command starts a Celery worker that will process tasks defined in your project. The `--loglevel=info` flag ensures you see task output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fce7bea-6671-4121-a188-6ddc650d1d73",
   "metadata": {},
   "source": [
    "### 4. **Defining and Executing Tasks**\n",
    "\n",
    "Tasks in Celery are just Python functions that you register as tasks using the `@task` decorator. You can create tasks in any app within the Django project.\n",
    "\n",
    "#### a. **Defining a Task**\n",
    "\n",
    "```python\n",
    "# tasks.py in any Django app\n",
    "\n",
    "from celery import shared_task\n",
    "from time import sleep\n",
    "\n",
    "@shared_task\n",
    "def add(x, y):\n",
    "    sleep(10)  # Simulate a time-consuming task\n",
    "    return x + y\n",
    "```\n",
    "\n",
    "- **`@shared_task`**: This decorator marks the function as a Celery task, meaning it can be executed asynchronously.\n",
    "\n",
    "#### b. **Calling a Task**\n",
    "\n",
    "To execute a Celery task asynchronously, you use the `delay()` method:\n",
    "\n",
    "```python\n",
    "from .tasks import add\n",
    "\n",
    "# Call the task asynchronously\n",
    "result = add.delay(4, 6)\n",
    "\n",
    "# You can also retrieve the result (if needed)\n",
    "print(result.get())  # This will block until the task is done\n",
    "```\n",
    "\n",
    "This will queue the `add` task, and Celery workers will execute it in the background.\n",
    "\n",
    "#### c. **Task Results**\n",
    "\n",
    "You can track task execution status and retrieve the result using the task’s ID:\n",
    "\n",
    "```python\n",
    "# Retrieve the result using the task ID\n",
    "from celery.result import AsyncResult\n",
    "\n",
    "result = AsyncResult(task_id)\n",
    "if result.successful():\n",
    "    print(result.result)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6827c9-4575-4e35-b84e-e50ccedab7fc",
   "metadata": {},
   "source": [
    "### 5. **Task Retries and Error Handling**\n",
    "\n",
    "Celery allows you to automatically retry tasks that fail due to transient errors.\n",
    "\n",
    "#### a. **Retrying a Task**\n",
    "\n",
    "To make a task retryable, use the `retry()` method in the task function:\n",
    "\n",
    "```python\n",
    "@shared_task(bind=True, max_retries=3)\n",
    "def example_task(self):\n",
    "    try:\n",
    "        # Simulate a task that may fail\n",
    "        risky_operation()\n",
    "    except SomeError as exc:\n",
    "        raise self.retry(exc=exc, countdown=5)\n",
    "```\n",
    "\n",
    "- **`max_retries=3`**: The task will be retried a maximum of 3 times.\n",
    "- **`countdown=5`**: The task will wait 5 seconds before retrying.\n",
    "\n",
    "#### b. **Error Handling**\n",
    "\n",
    "If a task raises an exception, it can be caught and retried or handled with a fallback:\n",
    "\n",
    "```python\n",
    "@shared_task(bind=True)\n",
    "def risky_task(self):\n",
    "    try:\n",
    "        risky_operation()\n",
    "    except Exception as exc:\n",
    "        self.update_state(state='FAILURE', meta={'error': str(exc)})\n",
    "        raise exc\n",
    "```\n",
    "\n",
    "- **`self.update_state()`**: Updates the task's state manually. You can store custom error metadata using the `meta` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56004826-b123-4261-b90e-565d63ab5472",
   "metadata": {},
   "source": [
    "### 6. **Periodic and Scheduled Tasks**\n",
    "\n",
    "You can schedule tasks to run at specific times or intervals, similar to cron jobs.\n",
    "\n",
    "#### a. **Using Celery Beat**\n",
    "\n",
    "**Celery Beat** is a scheduler that kicks off tasks at regular intervals. It works alongside your Celery workers.\n",
    "\n",
    "1. **Install the necessary package**:\n",
    "\n",
    "   ```bash\n",
    "   pip install django-celery-beat\n",
    "   ```\n",
    "\n",
    "2. **Add `django_celery_beat` to your `INSTALLED_APPS`** in `settings.py`:\n",
    "\n",
    "   ```python\n",
    "   INSTALLED_APPS = [\n",
    "       # other apps\n",
    "       'django_celery_beat',\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "3. **Run migrations** for Celery Beat:\n",
    "\n",
    "   ```bash\n",
    "   python manage.py migrate django_celery_beat\n",
    "   ```\n",
    "\n",
    "4. **Configure a Periodic Task** using the Django Admin or in code:\n",
    "\n",
    "   Example of a periodic task running every 10 minutes:\n",
    "\n",
    "   ```python\n",
    "   from django_celery_beat.models import PeriodicTask, IntervalSchedule\n",
    "\n",
    "   # Create an interval schedule (every 10 minutes)\n",
    "   schedule, created = IntervalSchedule.objects.get_or_create(\n",
    "       every=10,\n",
    "       period=IntervalSchedule.MINUTES,\n",
    "   )\n",
    "\n",
    "   # Create the periodic task\n",
    "   PeriodicTask.objects.create(\n",
    "       interval=schedule,                  # Use the schedule created above\n",
    "       name='My periodic task',\n",
    "       task='myapp.tasks.my_task',  # The name of the task function\n",
    "   )\n",
    "   ```\n",
    "\n",
    "5. **Start the Celery Beat Scheduler**:\n",
    "\n",
    "   ```bash\n",
    "   celery -A project beat --loglevel=info\n",
    "   ```\n",
    "\n",
    "   This command will run the scheduler, which triggers periodic tasks based on your schedule.\n",
    "\n",
    "#### b. **Using the `crontab` Scheduler**\n",
    "\n",
    "You can also configure tasks to run at specific times using cron-like syntax:\n",
    "\n",
    "```python\n",
    "from celery.schedules import crontab\n",
    "\n",
    "app.conf.beat_schedule = {\n",
    "    'task_name': {\n",
    "        'task': 'myapp.tasks.my_task',\n",
    "        'schedule': crontab(hour=7, minute=30, day_of_week=1),  # Every Monday at 7:30 AM\n",
    "    },\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c7fb7c-fa50-4d31-96ef-fe589a2358b7",
   "metadata": {},
   "source": [
    "### 7. **Optimizing Celery for Production**\n",
    "\n",
    "#### a. **Concurrency**\n",
    "You can control how many worker processes or threads Celery spawns using the `--concurrency` option when starting workers:\n",
    "\n",
    "```bash\n",
    "celery -A project worker --concurrency=4\n",
    "```\n",
    "\n",
    "This will start 4 worker processes.\n",
    "\n",
    "#### b. **Prefetch Limit**\n",
    "Celery workers can fetch multiple tasks in advance, but this may overload workers in some cases. You can limit the number of prefetched tasks with `CELERYD_PREFETCH_MULTIPLIER`:\n",
    "\n",
    "```python\n",
    "app.conf.worker_prefetch_multiplier = 1  # Pref\n",
    "\n",
    "etch only 1 task at a time\n",
    "```\n",
    "\n",
    "#### c. **Task Time Limits**\n",
    "If a task takes too long, you can set time limits to prevent worker exhaustion:\n",
    "\n",
    "```python\n",
    "app.conf.task_soft_time_limit = 60  # Raise exception after 60 seconds\n",
    "app.conf.task_time_limit = 120      # Hard limit (kill task after 120 seconds)\n",
    "```\n",
    "\n",
    "#### d. **Monitoring and Admin**\n",
    "Use **Flower**, a web-based tool to monitor Celery workers and tasks:\n",
    "\n",
    "1. Install Flower:\n",
    "\n",
    "   ```bash\n",
    "   pip install flower\n",
    "   ```\n",
    "\n",
    "2. Start Flower:\n",
    "\n",
    "   ```bash\n",
    "   celery -A project flower\n",
    "   ```\n",
    "\n",
    "   Flower provides a web interface that shows the current status of workers, tasks, and queues. You can view task progress, task failures, and retry counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3d36a-97c7-4b13-bdb8-485d588ed1d0",
   "metadata": {},
   "source": [
    "### 8. **Broker Options**\n",
    "\n",
    "Celery supports a wide range of message brokers, including:\n",
    "\n",
    "- **Redis**: A simple key-value store that can act as a message broker.\n",
    "- **RabbitMQ**: A robust, feature-rich message broker that supports advanced features like routing and exchanges.\n",
    "- **Amazon SQS**: A scalable message queue service.\n",
    "\n",
    "#### Redis Example:\n",
    "\n",
    "```python\n",
    "CELERY_BROKER_URL = 'redis://localhost:6379/0'\n",
    "CELERY_RESULT_BACKEND = 'redis://localhost:6379/0'\n",
    "```\n",
    "\n",
    "#### RabbitMQ Example:\n",
    "\n",
    "```python\n",
    "CELERY_BROKER_URL = 'amqp://guest:guest@localhost:5672//'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c2e939-6fa8-4f6c-a152-9e6797661c46",
   "metadata": {},
   "source": [
    "### 9. **Best Practices**\n",
    "\n",
    "- **Use Task Queues**: Use task queues to categorize and separate different types of tasks, especially if they have different priority or resource requirements.\n",
    "  \n",
    "  Example:\n",
    "  ```python\n",
    "  celery -A project worker -Q queue_name\n",
    "  ```\n",
    "\n",
    "- **Idempotent Tasks**: Ensure tasks are idempotent, meaning they can run multiple times without unintended side effects. This is essential because Celery retries tasks after failures.\n",
    "  \n",
    "- **Error Handling**: Always handle potential errors inside tasks using `try/except` and consider using `retry()` for transient issues.\n",
    "\n",
    "- **Monitor Tasks**: Use monitoring tools like **Flower** or **Prometheus** to track worker health and task performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
